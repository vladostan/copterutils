{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import matplotlib.pylab as plt\n",
    "from glob import glob\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Datasets used: ['C:\\\\Users\\\\vlado\\\\Desktop\\\\copterutils\\\\seg\\\\data\\\\images/ds1', 'C:\\\\Users\\\\vlado\\\\Desktop\\\\copterutils\\\\seg\\\\data\\\\images/ds1/albumentated']\n",
      "\n"
     ]
    }
   ],
   "source": [
    "PATH = os.path.abspath('data')\n",
    "\n",
    "SOURCE_IMAGES = [os.path.join(PATH, \"images/ds1\"), \n",
    "                 os.path.join(PATH, \"images/ds1/albumentated\")]\n",
    "\n",
    "images = []\n",
    "labels = []\n",
    "\n",
    "for si in SOURCE_IMAGES:\n",
    "    images.extend(glob(os.path.join(si, \"*.jpg\")))\n",
    "    labels.extend(glob(os.path.join(si.replace(\"images\",\"labels\"), \"*.png\")))\n",
    "    \n",
    "print(\"Datasets used: {}\\n\".format(SOURCE_IMAGES))\n",
    "\n",
    "images.sort()\n",
    "labels.sort()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_image(path):\n",
    "    \n",
    "    image = plt.imread(path)\n",
    "    \n",
    "    return(np.asarray(image))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_label(path):\n",
    "\n",
    "    label = plt.imread(path, 0)\n",
    "    \n",
    "    return(np.asarray(label))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'numpy.ndarray' object has no attribute 'type'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-125-2e2bf4b723a5>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mget_image\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mimages\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtype\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m: 'numpy.ndarray' object has no attribute 'type'"
     ]
    }
   ],
   "source": [
    "get_image(images[0]).type"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 248,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_input(x):\n",
    "    \n",
    "    # Crop squared image\n",
    "    h, w = x.shape[:2]\n",
    "\n",
    "    x = x[:,(w-h)//2:(w-(w-h)//2),:]\n",
    "    \n",
    "    h, w = x.shape[:2]\n",
    "    \n",
    "    # # Split images and masks into batches (optional)\n",
    "    # In[23]:\n",
    "    h_t, w_t = (512,512)\n",
    "    split_factor = 4 # We crop this number of smaller images out of one\n",
    "    x_t = np.zeros([split_factor, h_t, w_t, 3], dtype='uint8')\n",
    "    \n",
    "    x_t[0] = x[:h//2,:w//2,:]\n",
    "    x_t[1] = x[:h//2,w//2:w,:]\n",
    "    x_t[2] = x[h//2:h,:w//2,:]\n",
    "    x_t[3] = x[h//2:h,w//2:w,:]\n",
    "    \n",
    "    x_t = np.float32(x_t/255.)\n",
    "\n",
    "    return(x_t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 249,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = get_image(images[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 250,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = preprocess_input(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 251,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(4, 512, 512, 3)"
      ]
     },
     "execution_count": 251,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 252,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.utils import to_categorical\n",
    "\n",
    "def preprocess_output(y):\n",
    "    \n",
    "    # Crop squared image\n",
    "    h, w = y.shape[:2]\n",
    "\n",
    "    y = y[:,(w-h)//2:(w-(w-h)//2)]\n",
    "    \n",
    "    h, w = y.shape[:2]\n",
    "    \n",
    "        # # Split images and masks into batches (optional)\n",
    "    # In[23]:\n",
    "    h_t, w_t = (512,512)\n",
    "    split_factor = 4 # We crop this number of smaller images out of one\n",
    "    y_t = np.zeros([split_factor, h_t, w_t], dtype='uint8')\n",
    "    \n",
    "    y_t[0] = y[:h//2,:w//2]\n",
    "    y_t[1] = y[:h//2,w//2:w]\n",
    "    y_t[2] = y[h//2:h,:w//2]\n",
    "    y_t[3] = y[h//2:h,w//2:w]\n",
    "    \n",
    "    y_t = to_categorical(y_t, num_classes=9)\n",
    "    y_t = y_t.reshape(y_t.shape[:3] + (9,))\n",
    "    y_t = y_t.astype('int8')\n",
    "\n",
    "    return(y_t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 253,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = get_label(labels[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 254,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = preprocess_output(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 255,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(4, 512, 512, 9)"
      ]
     },
     "execution_count": 255,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "test_size = 0.2\n",
    "images_train, images_test, labels_train, labels_test = train_test_split(images, labels, test_size=test_size, random_state=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 261,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "114\n",
      "114\n",
      "29\n",
      "29\n"
     ]
    }
   ],
   "source": [
    "print(len(images_train))\n",
    "print(len(labels_train))\n",
    "print(len(images_test))\n",
    "print(len(labels_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from albumentations import (\n",
    "    PadIfNeeded,\n",
    "    HorizontalFlip,\n",
    "    VerticalFlip,\n",
    "    ShiftScaleRotate,\n",
    "    CenterCrop,    \n",
    "    Crop,\n",
    "    Compose,\n",
    "    Transpose,\n",
    "    Rotate,\n",
    "    RandomRotate90,\n",
    "    ElasticTransform,\n",
    "    GridDistortion, \n",
    "    OpticalDistortion,\n",
    "    RandomSizedCrop,\n",
    "    OneOf,\n",
    "    CLAHE,\n",
    "    RandomContrast,\n",
    "    RandomGamma,\n",
    "    RandomBrightness\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def augment(image, mask):\n",
    "\n",
    "    original_height, original_width = image.shape[:2]\n",
    "\n",
    "    aug = Compose([\n",
    "        RandomSizedCrop(p=0.5, min_max_height=(original_height//2, original_height), height=original_height, width=original_width),\n",
    "        OpticalDistortion(p=0.5, distort_limit=0.25, shift_limit=0.5),\n",
    "        OneOf([\n",
    "            CLAHE(p=1., clip_limit=4.),\n",
    "            RandomContrast(p=1., limit=0.25),\n",
    "            RandomGamma(p=1., gamma_limit=(50,200))\n",
    "            ], p=0.5),\n",
    "        ])\n",
    "\n",
    "    augmented = aug(image=image, mask=mask)\n",
    "\n",
    "    image_heavy = augmented['image']\n",
    "    mask_heavy = augmented['mask']\n",
    "    \n",
    "    return image_heavy, mask_heavy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 256,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generator(images_path, labels_path, batch_size = 2):\n",
    "    \n",
    "#     while True:\n",
    "        ids = np.random.randint(0, len(images_path), batch_size)\n",
    "        \n",
    "        image_batch = np.take(images_path, ids)\n",
    "        label_batch = np.take(labels_path, ids)\n",
    "        \n",
    "        batch_input = []\n",
    "        batch_output = [] \n",
    "\n",
    "        # Read in each input, perform preprocessing and get labels\n",
    "        for input_path in image_batch:\n",
    "            inp = get_image(input_path)\n",
    "            inp = preprocess_input(inp)\n",
    "            batch_input.extend(inp)\n",
    "\n",
    "        for input_path in label_batch:\n",
    "            outp = get_label(input_path)\n",
    "            outp = preprocess_output(outp)\n",
    "            batch_output.extend(outp)\n",
    "\n",
    "        # Return a tuple of (input,output) to feed the network\n",
    "        batch_x = np.array(batch_input)\n",
    "        batch_y = np.array(batch_output)\n",
    "\n",
    "#         yield(batch_x, batch_y)      \n",
    "        return(batch_x, batch_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 257,
   "metadata": {},
   "outputs": [],
   "source": [
    "generator = generator(images_path=images, labels_path=labels, batch_size=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 260,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(8, 512, 512, 3)"
      ]
     },
     "execution_count": 260,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "generator[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "\n",
    "# datagen = ImageDataGenerator(contrast_stretching=True, adaptive_equalization=True, histogram_equalization=True)\n",
    "datagen = ImageDataGenerator()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fit parameters from data\n",
    "datagen.fit\n",
    "\n",
    "# Configure batch size and retrieve one batch of images\n",
    "for x_batch, y_batch in datagen.flow(x, y, batch_size=9):\n",
    "    # Show the first 9 images\n",
    "    for i in range(0, 9):\n",
    "        pyplot.subplot(330 + 1 + i)\n",
    "        pyplot.imshow(x_batch[i].reshape(img_rows, img_cols, 3))\n",
    "    # show the plot\n",
    "    pyplot.show()\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model summary:\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_2 (InputLayer)            (None, 512, 512, 3)  0                                            \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_24 (Conv2D)              (None, 512, 512, 64) 1792        input_2[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_25 (Conv2D)              (None, 512, 512, 64) 36928       conv2d_24[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling2d_5 (MaxPooling2D)  (None, 256, 256, 64) 0           conv2d_25[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_26 (Conv2D)              (None, 256, 256, 128 73856       max_pooling2d_5[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_27 (Conv2D)              (None, 256, 256, 128 147584      conv2d_26[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling2d_6 (MaxPooling2D)  (None, 128, 128, 128 0           conv2d_27[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_28 (Conv2D)              (None, 128, 128, 256 295168      max_pooling2d_6[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_29 (Conv2D)              (None, 128, 128, 256 590080      conv2d_28[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling2d_7 (MaxPooling2D)  (None, 64, 64, 256)  0           conv2d_29[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_30 (Conv2D)              (None, 64, 64, 512)  1180160     max_pooling2d_7[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_31 (Conv2D)              (None, 64, 64, 512)  2359808     conv2d_30[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dropout_3 (Dropout)             (None, 64, 64, 512)  0           conv2d_31[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling2d_8 (MaxPooling2D)  (None, 32, 32, 512)  0           dropout_3[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_32 (Conv2D)              (None, 32, 32, 1024) 4719616     max_pooling2d_8[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_33 (Conv2D)              (None, 32, 32, 1024) 9438208     conv2d_32[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dropout_4 (Dropout)             (None, 32, 32, 1024) 0           conv2d_33[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "up_sampling2d_5 (UpSampling2D)  (None, 64, 64, 1024) 0           dropout_4[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_34 (Conv2D)              (None, 64, 64, 512)  2097664     up_sampling2d_5[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_5 (Concatenate)     (None, 64, 64, 1024) 0           dropout_3[0][0]                  \n",
      "                                                                 conv2d_34[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_35 (Conv2D)              (None, 64, 64, 512)  4719104     concatenate_5[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_36 (Conv2D)              (None, 64, 64, 512)  2359808     conv2d_35[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "up_sampling2d_6 (UpSampling2D)  (None, 128, 128, 512 0           conv2d_36[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_37 (Conv2D)              (None, 128, 128, 256 524544      up_sampling2d_6[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_6 (Concatenate)     (None, 128, 128, 512 0           conv2d_29[0][0]                  \n",
      "                                                                 conv2d_37[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_38 (Conv2D)              (None, 128, 128, 256 1179904     concatenate_6[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_39 (Conv2D)              (None, 128, 128, 256 590080      conv2d_38[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "up_sampling2d_7 (UpSampling2D)  (None, 256, 256, 256 0           conv2d_39[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_40 (Conv2D)              (None, 256, 256, 128 131200      up_sampling2d_7[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_7 (Concatenate)     (None, 256, 256, 256 0           conv2d_27[0][0]                  \n",
      "                                                                 conv2d_40[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_41 (Conv2D)              (None, 256, 256, 128 295040      concatenate_7[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_42 (Conv2D)              (None, 256, 256, 128 147584      conv2d_41[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "up_sampling2d_8 (UpSampling2D)  (None, 512, 512, 128 0           conv2d_42[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_43 (Conv2D)              (None, 512, 512, 64) 32832       up_sampling2d_8[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_8 (Concatenate)     (None, 512, 512, 128 0           conv2d_25[0][0]                  \n",
      "                                                                 conv2d_43[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_44 (Conv2D)              (None, 512, 512, 64) 73792       concatenate_8[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_45 (Conv2D)              (None, 512, 512, 64) 36928       conv2d_44[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_46 (Conv2D)              (None, 512, 512, 9)  585         conv2d_45[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_2 (Activation)       (None, 512, 512, 9)  0           conv2d_46[0][0]                  \n",
      "==================================================================================================\n",
      "Total params: 31,032,265\n",
      "Trainable params: 31,032,265\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n",
      "Optimizer: <keras.optimizers.Adam object at 0x000001FF199D7630>, learning rate: 0.0001, loss: categorical_crossentropy, metrics: ['accuracy']\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from models.Unet import unet\n",
    "\n",
    "model = unet(input_size = (512,512,3), n_classes=9)\n",
    "\n",
    "print(\"Model summary:\")\n",
    "model.summary()\n",
    "\n",
    "# In[ ]:\n",
    "from keras import optimizers\n",
    "\n",
    "#model.compile(optimizer = 'adadelta', loss = 'categorical_crossentropy', metrics = ['accuracy'])\n",
    "\n",
    "learning_rate = 1e-4\n",
    "optimizer = optimizers.Adam(lr = learning_rate)\n",
    "loss = 'categorical_crossentropy'\n",
    "metrics = ['accuracy']\n",
    "\n",
    "print(\"Optimizer: {}, learning rate: {}, loss: {}, metrics: {}\\n\".format(optimizer, learning_rate, loss, metrics))\n",
    "\n",
    "model.compile(optimizer = optimizer, loss = loss, metrics = metrics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/1\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Error when checking input: expected input_2 to have 4 dimensions, but got array with shape (2, 4, 512, 512, 3)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-197-9744cbfef186>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      3\u001b[0m     \u001b[0msteps_per_epoch\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m16\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m     \u001b[0mepochs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 5\u001b[1;33m     \u001b[0mverbose\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      6\u001b[0m )\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\keras\\legacy\\interfaces.py\u001b[0m in \u001b[0;36mwrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     89\u001b[0m                 warnings.warn('Update your `' + object_name + '` call to the ' +\n\u001b[0;32m     90\u001b[0m                               'Keras 2 API: ' + signature, stacklevel=2)\n\u001b[1;32m---> 91\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     92\u001b[0m         \u001b[0mwrapper\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_original_function\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     93\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\keras\\engine\\training.py\u001b[0m in \u001b[0;36mfit_generator\u001b[1;34m(self, generator, steps_per_epoch, epochs, verbose, callbacks, validation_data, validation_steps, class_weight, max_queue_size, workers, use_multiprocessing, shuffle, initial_epoch)\u001b[0m\n\u001b[0;32m   1416\u001b[0m             \u001b[0muse_multiprocessing\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0muse_multiprocessing\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1417\u001b[0m             \u001b[0mshuffle\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mshuffle\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1418\u001b[1;33m             initial_epoch=initial_epoch)\n\u001b[0m\u001b[0;32m   1419\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1420\u001b[0m     \u001b[1;33m@\u001b[0m\u001b[0minterfaces\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlegacy_generator_methods_support\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\keras\\engine\\training_generator.py\u001b[0m in \u001b[0;36mfit_generator\u001b[1;34m(model, generator, steps_per_epoch, epochs, verbose, callbacks, validation_data, validation_steps, class_weight, max_queue_size, workers, use_multiprocessing, shuffle, initial_epoch)\u001b[0m\n\u001b[0;32m    215\u001b[0m                 outs = model.train_on_batch(x, y,\n\u001b[0;32m    216\u001b[0m                                             \u001b[0msample_weight\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0msample_weight\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 217\u001b[1;33m                                             class_weight=class_weight)\n\u001b[0m\u001b[0;32m    218\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    219\u001b[0m                 \u001b[0mouts\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mto_list\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mouts\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\keras\\engine\\training.py\u001b[0m in \u001b[0;36mtrain_on_batch\u001b[1;34m(self, x, y, sample_weight, class_weight)\u001b[0m\n\u001b[0;32m   1209\u001b[0m             \u001b[0mx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1210\u001b[0m             \u001b[0msample_weight\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0msample_weight\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1211\u001b[1;33m             class_weight=class_weight)\n\u001b[0m\u001b[0;32m   1212\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_uses_dynamic_learning_phase\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1213\u001b[0m             \u001b[0mins\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mx\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0my\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0msample_weights\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;36m1.\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\keras\\engine\\training.py\u001b[0m in \u001b[0;36m_standardize_user_data\u001b[1;34m(self, x, y, sample_weight, class_weight, check_array_lengths, batch_size)\u001b[0m\n\u001b[0;32m    749\u001b[0m             \u001b[0mfeed_input_shapes\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    750\u001b[0m             \u001b[0mcheck_batch_axis\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m,\u001b[0m  \u001b[1;31m# Don't enforce the batch size.\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 751\u001b[1;33m             exception_prefix='input')\n\u001b[0m\u001b[0;32m    752\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    753\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0my\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\keras\\engine\\training_utils.py\u001b[0m in \u001b[0;36mstandardize_input_data\u001b[1;34m(data, names, shapes, check_batch_axis, exception_prefix)\u001b[0m\n\u001b[0;32m    126\u001b[0m                         \u001b[1;34m': expected '\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mnames\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;34m' to have '\u001b[0m \u001b[1;33m+\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    127\u001b[0m                         \u001b[0mstr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;34m' dimensions, but got array '\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 128\u001b[1;33m                         'with shape ' + str(data_shape))\n\u001b[0m\u001b[0;32m    129\u001b[0m                 \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mcheck_batch_axis\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    130\u001b[0m                     \u001b[0mdata_shape\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdata_shape\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: Error when checking input: expected input_2 to have 4 dimensions, but got array with shape (2, 4, 512, 512, 3)"
     ]
    }
   ],
   "source": [
    "history = model.fit_generator(\n",
    "    generator,\n",
    "    steps_per_epoch = 16,\n",
    "    epochs = 1,\n",
    "    verbose = 1\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
